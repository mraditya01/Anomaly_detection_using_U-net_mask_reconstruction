{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.8.1)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.56.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting soundfile>=0.10.2\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa) (5.1.1)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.23.1)\n",
      "Collecting setuptools<60\n",
      "  Downloading setuptools-59.8.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.8/952.8 kB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23702 sha256=80700e569808b367be490b58fa9617dc9fd2f31b4e0db3e8ddf2791d13f7494f\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/76/a4/cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "Successfully built audioread\n",
      "Installing collected packages: appdirs, setuptools, llvmlite, audioread, soundfile, pooch, numba, resampy, librosa\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.1.0\n",
      "    Uninstalling setuptools-63.1.0:\n",
      "      Successfully uninstalled setuptools-63.1.0\n",
      "Successfully installed appdirs-1.4.4 audioread-3.0.0 librosa-0.9.2 llvmlite-0.39.1 numba-0.56.2 pooch-1.6.0 resampy-0.4.2 setuptools-59.8.0 soundfile-0.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# import default libraries\n",
    "########################################################################\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional libraries\n",
    "########################################################################\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "# from import\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "except:\n",
    "    import joblib\n",
    "# original lib\n",
    "import common as com\n",
    "import keras_model\n",
    "import pandas as pd\n",
    "import keras\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# load parameter.yaml\n",
    "########################################################################\n",
    "param = com.yaml_load()\n",
    "########################################################################\n",
    "saved_weight = os.path.join(param[\"P_MODELSAVE\"], 'dataweights.{epoch:02d}-{val_acc:.2f}.hdf5')\n",
    "\n",
    "modelchk = keras.callbacks.ModelCheckpoint(saved_weight, \n",
    "                                      monitor='val_acc', \n",
    "                                      verbose=1,\n",
    "                                      save_best_only=True, \n",
    "                                      save_weights_only=False,\n",
    "                                      mode='auto',\n",
    "                                      period=2)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=param[\"P_LOGS\"],\n",
    "                                          histogram_freq=0,\n",
    "                                          write_graph=True,\n",
    "                                          write_images=True)\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(f'{param[\"P_LOGS\"]}/keras_log.csv',\n",
    "                                       append=True)\n",
    "\n",
    "# model_unet = keras_model.get_model(input_shape=(1,param[\"feature\"][\"n_mels\"],param[\"feature\"][\"n_frames\"]), lr = param[\"fit\"][\"lr\"])\n",
    "\n",
    "########################################################################\n",
    "# visualizer\n",
    "########################################################################\n",
    "class visualizer(object):\n",
    "    def __init__(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.plt = plt\n",
    "        self.fig = self.plt.figure(figsize=(7, 5))\n",
    "        self.plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    def loss_plot(self, loss, val_loss):\n",
    "        \"\"\"\n",
    "        Plot loss curve.\n",
    "\n",
    "        loss : list [ float ]\n",
    "            training loss time series.\n",
    "        val_loss : list [ float ]\n",
    "            validation loss time series.\n",
    "\n",
    "        return   : None\n",
    "        \"\"\"\n",
    "        ax = self.fig.add_subplot(1, 1, 1)\n",
    "        ax.cla()\n",
    "        ax.plot(loss)\n",
    "        ax.plot(val_loss)\n",
    "        ax.set_title(\"Model loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "    def save_figure(self, name):\n",
    "        \"\"\"\n",
    "        Save figure.\n",
    "\n",
    "        name : str\n",
    "            save png file path.\n",
    "\n",
    "        return : None\n",
    "        \"\"\"\n",
    "        self.plt.savefig(name)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# get data from the list for file paths\n",
    "########################################################################\n",
    "def file_list_to_data(file_list,\n",
    "                      msg=\"calc...\",\n",
    "                      n_mels=64,\n",
    "                      n_frames=5,\n",
    "                      n_hop_frames=1,\n",
    "                      n_fft=1024,\n",
    "                      hop_length=512,\n",
    "                      power=2.0):\n",
    "    \"\"\"\n",
    "    convert the file_list to a vector array.\n",
    "    file_to_vector_array() is iterated, and the output vector array is concatenated.\n",
    "\n",
    "    file_list : list [ str ]\n",
    "        .wav filename list of dataset\n",
    "    msg : str ( default = \"calc...\" )\n",
    "        description for tqdm.\n",
    "        this parameter will be input into \"desc\" param at tqdm.\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        data for training (this function is not used for test.)\n",
    "        * dataset.shape = (number of feature vectors, dimensions of feature vectors)\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate file_to_vector_array()\n",
    "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
    "        vectors = com.file_to_vectors(file_list[idx],\n",
    "                                                n_mels=n_mels,\n",
    "                                                n_frames=n_frames,\n",
    "                                                n_fft=n_fft,\n",
    "                                                hop_length=hop_length,\n",
    "                                                power=power)\n",
    "        # vectors_masked = com.spec_augment(vectors)\n",
    "        if idx == 0:\n",
    "            data = np.zeros((len(file_list), vectors.shape[0], vectors.shape[1]), float)\n",
    "            # data_masked = np.zeros((len(file_list), vectors_masked.shape[1], vectors_masked.shape[0]), float)\n",
    "        data[idx, :, :] = vectors\n",
    "        data[idx,:,:] = librosa.power_to_db(data[idx,:,:])\n",
    "        # data_masked[idx, :, :] = vectors_masked.T\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 08:56:43,733 - INFO - load_directory <- development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "[1/8] dev_data/car3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "# initialize the visualizer\n",
    "visualizer = visualizer()\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car3\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 08:56:46,614 - INFO - target_dir : dev_data/car3_*\n",
      "2022-10-17 08:56:46,625 - INFO - #files : 611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DATASET_GENERATOR ==============\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 611/611 [00:12<00:00, 48.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(611, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# j = param[\"feature\"][\"n_mels\"]\n",
    "# y = param[\"feature\"][\"n_frames\"]\n",
    "# data = np.load(f\"data__{j}_{y}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_mels\"], param[\"feature\"][\"n_frames\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mae', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7d30985f23d0c7700ff9117488976e591efd57aed74d4a8ca9ee5ad3f3e7e0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

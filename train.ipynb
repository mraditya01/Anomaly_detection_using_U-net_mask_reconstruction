{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.8.1)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (21.3)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.0)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.56.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools<60\n",
      "  Downloading setuptools-59.8.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.8/952.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23702 sha256=f32eb7c8a64701fa18c4a25b8b97884c22d0b8bb801c15b911248e63965f0271\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/76/a4/cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "Successfully built audioread\n",
      "Installing collected packages: appdirs, setuptools, llvmlite, audioread, soundfile, pooch, numba, resampy, librosa\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.1.0\n",
      "    Uninstalling setuptools-63.1.0:\n",
      "      Successfully uninstalled setuptools-63.1.0\n",
      "Successfully installed appdirs-1.4.4 audioread-3.0.0 librosa-0.9.2 llvmlite-0.39.1 numba-0.56.2 pooch-1.6.0 resampy-0.4.2 setuptools-59.8.0 soundfile-0.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# import default libraries\n",
    "########################################################################\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional libraries\n",
    "########################################################################\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "# from import\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "except:\n",
    "    import joblib\n",
    "# original lib\n",
    "import common as com\n",
    "import keras_model\n",
    "import pandas as pd\n",
    "import keras\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# load parameter.yaml\n",
    "########################################################################\n",
    "param = com.yaml_load()\n",
    "########################################################################\n",
    "saved_weight = os.path.join(param[\"P_MODELSAVE\"], 'dataweights.{epoch:02d}-{val_acc:.2f}.hdf5')\n",
    "\n",
    "modelchk = keras.callbacks.ModelCheckpoint(saved_weight, \n",
    "                                      monitor='val_acc', \n",
    "                                      verbose=1,\n",
    "                                      save_best_only=True, \n",
    "                                      save_weights_only=False,\n",
    "                                      mode='auto',\n",
    "                                      period=2)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=param[\"P_LOGS\"],\n",
    "                                          histogram_freq=0,\n",
    "                                          write_graph=True,\n",
    "                                          write_images=True)\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(f'{param[\"P_LOGS\"]}/keras_log.csv',\n",
    "                                       append=True)\n",
    "\n",
    "# model_unet = keras_model.get_model(input_shape=(1,param[\"feature\"][\"n_mels\"],param[\"feature\"][\"n_frames\"]), lr = param[\"fit\"][\"lr\"])\n",
    "\n",
    "########################################################################\n",
    "# visualizer\n",
    "########################################################################\n",
    "class visualizer(object):\n",
    "    def __init__(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.plt = plt\n",
    "        self.fig = self.plt.figure(figsize=(7, 5))\n",
    "        self.plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    def loss_plot(self, loss, val_loss):\n",
    "        \"\"\"\n",
    "        Plot loss curve.\n",
    "\n",
    "        loss : list [ float ]\n",
    "            training loss time series.\n",
    "        val_loss : list [ float ]\n",
    "            validation loss time series.\n",
    "\n",
    "        return   : None\n",
    "        \"\"\"\n",
    "        ax = self.fig.add_subplot(1, 1, 1)\n",
    "        ax.cla()\n",
    "        ax.plot(loss)\n",
    "        ax.plot(val_loss)\n",
    "        ax.set_title(\"Model loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "    def save_figure(self, name):\n",
    "        \"\"\"\n",
    "        Save figure.\n",
    "\n",
    "        name : str\n",
    "            save png file path.\n",
    "\n",
    "        return : None\n",
    "        \"\"\"\n",
    "        self.plt.savefig(name)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# get data from the list for file paths\n",
    "########################################################################\n",
    "def file_list_to_data(file_list,\n",
    "                      msg=\"calc...\",\n",
    "                      n_mels=64,\n",
    "                      n_frames=5,\n",
    "                      n_hop_frames=1,\n",
    "                      n_fft=1024,\n",
    "                      hop_length=512,\n",
    "                      power=2.0):\n",
    "    \"\"\"\n",
    "    convert the file_list to a vector array.\n",
    "    file_to_vector_array() is iterated, and the output vector array is concatenated.\n",
    "\n",
    "    file_list : list [ str ]\n",
    "        .wav filename list of dataset\n",
    "    msg : str ( default = \"calc...\" )\n",
    "        description for tqdm.\n",
    "        this parameter will be input into \"desc\" param at tqdm.\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        data for training (this function is not used for test.)\n",
    "        * dataset.shape = (number of feature vectors, dimensions of feature vectors)\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate file_to_vector_array()\n",
    "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
    "        vectors = com.file_to_vectors(file_list[idx],\n",
    "                                                n_mels=n_mels,\n",
    "                                                n_frames=n_frames,\n",
    "                                                n_fft=n_fft,\n",
    "                                                hop_length=hop_length,\n",
    "                                                power=power)\n",
    "        # vectors_masked = com.spec_augment(vectors)\n",
    "        if idx == 0:\n",
    "            data = np.zeros((len(file_list), vectors.shape[0], vectors.shape[1]), float)\n",
    "            # data_masked = np.zeros((len(file_list), vectors_masked.shape[1], vectors_masked.shape[0]), float)\n",
    "        data[idx, :, :] = vectors\n",
    "        data[idx,:,:] = librosa.power_to_db(data[idx,:,:])\n",
    "        # data_masked[idx, :, :] = vectors_masked.T\n",
    "    return data\n",
    "\n",
    "def masking(data):\n",
    "      for idx in range(len(data)):\n",
    "        \n",
    "        vectors_masked = com.spec_augment(data[idx,:,:])\n",
    "        # data[idx,:,:] = librosa.power_to_db(data[idx,:,:])\n",
    "        # vectors_masked[idx,:,:] = librosa.power_to_db(vectors_masked[idx,:,:])\n",
    "        if idx == 0:\n",
    "            data_masked = np.zeros((len(data), vectors_masked.shape[0], vectors_masked.shape[1]), float)\n",
    "        data_masked[idx, :, :] = vectors_masked\n",
    "      return np.swapaxes(data_masked, 1, 2), np.swapaxes(data, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 23:19:46,471 - INFO - load_directory <- development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "[1/8] dev_data/car3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "# initialize the visualizer\n",
    "visualizer = visualizer()\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car3\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 23:19:46,494 - INFO - target_dir : dev_data/car3_*\n",
      "2022-10-11 23:19:46,502 - INFO - #files : 611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DATASET_GENERATOR ==============\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 611/611 [00:14<00:00, 43.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# j = param[\"feature\"][\"n_mels\"]\n",
    "# y = param[\"feature\"][\"n_frames\"]\n",
    "# data = np.load(f\"data__{j}_{y}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 512, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_masked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 512, 128, 1)\n",
      "(1222, 512, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data_masked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 512, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=2)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 23:43:16,381 - INFO - load_directory <- development\n",
      "2022-10-11 23:43:16,389 - INFO - target_dir : dev_data/car4_*\n",
      "2022-10-11 23:43:16,395 - INFO - #files : 611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "[1/8] dev_data/car4\n",
      "============== DATASET_GENERATOR ==============\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 611/611 [00:16<00:00, 37.20it/s]\n"
     ]
    }
   ],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car4\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 512, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " block_1_conv (Conv2D)          (None, 512, 128, 16  160         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_conv_bn (BatchNormaliz  (None, 512, 128, 16  64         ['block_1_conv[0][0]']           \n",
      " ation)                         )                                                                 \n",
      "                                                                                                  \n",
      " block_1_lrelu (LeakyReLU)      (None, 512, 128, 16  0           ['block_1_conv_bn[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)        (None, 256, 64, 16)  0           ['block_1_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_2_conv (Conv2D)          (None, 256, 64, 32)  4640        ['maxpool1[0][0]']               \n",
      "                                                                                                  \n",
      " block_2_conv_bn (BatchNormaliz  (None, 256, 64, 32)  128        ['block_2_conv[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_2_lrelu (LeakyReLU)      (None, 256, 64, 32)  0           ['block_2_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)        (None, 128, 32, 32)  0           ['block_2_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_3_conv (Conv2D)          (None, 128, 32, 64)  18496       ['maxpool2[0][0]']               \n",
      "                                                                                                  \n",
      " block_3_conv_bn (BatchNormaliz  (None, 128, 32, 64)  256        ['block_3_conv[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_3_lrelu (LeakyReLU)      (None, 128, 32, 64)  0           ['block_3_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " maxpool3 (MaxPooling2D)        (None, 64, 16, 64)   0           ['block_3_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_4_conv (Conv2D)          (None, 64, 16, 128)  73856       ['maxpool3[0][0]']               \n",
      "                                                                                                  \n",
      " block_4_conv_bn (BatchNormaliz  (None, 64, 16, 128)  512        ['block_4_conv[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_4_lrelu (LeakyReLU)      (None, 64, 16, 128)  0           ['block_4_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " maxpool4 (MaxPooling2D)        (None, 32, 8, 128)   0           ['block_4_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_5_conv (Conv2D)          (None, 32, 8, 256)   295168      ['maxpool4[0][0]']               \n",
      "                                                                                                  \n",
      " block_5_conv_bn (BatchNormaliz  (None, 32, 8, 256)  1024        ['block_5_conv[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_5_lrelu (LeakyReLU)      (None, 32, 8, 256)   0           ['block_5_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " block_6_de-conv (Conv2DTranspo  (None, 64, 16, 128)  295040     ['block_5_lrelu[0][0]']          \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " block_6_conv_bn (BatchNormaliz  (None, 64, 16, 128)  512        ['block_6_de-conv[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_6_lrelu (LeakyReLU)      (None, 64, 16, 128)  0           ['block_6_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " block_6_drop (Dropout)         (None, 64, 16, 128)  0           ['block_6_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " skip1 (Concatenate)            (None, 64, 16, 256)  0           ['block_6_drop[0][0]',           \n",
      "                                                                  'block_4_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_7_conv (Conv2D)          (None, 64, 16, 128)  295040      ['skip1[0][0]']                  \n",
      "                                                                                                  \n",
      " block_7_conv_bn (BatchNormaliz  (None, 64, 16, 128)  512        ['block_7_conv[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_7_lrelu (LeakyReLU)      (None, 64, 16, 128)  0           ['block_7_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " block_8_de-conv (Conv2DTranspo  (None, 128, 32, 64)  73792      ['block_7_lrelu[0][0]']          \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " block_8_conv_bn (BatchNormaliz  (None, 128, 32, 64)  256        ['block_8_de-conv[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_8_lrelu (LeakyReLU)      (None, 128, 32, 64)  0           ['block_8_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " block_8_drop (Dropout)         (None, 128, 32, 64)  0           ['block_8_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " skip2 (Concatenate)            (None, 128, 32, 128  0           ['block_8_drop[0][0]',           \n",
      "                                )                                 'block_3_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_9_conv (Conv2D)          (None, 128, 32, 64)  73792       ['skip2[0][0]']                  \n",
      "                                                                                                  \n",
      " block_9_conv_bn (BatchNormaliz  (None, 128, 32, 64)  256        ['block_9_conv[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " block_9_lrelu (LeakyReLU)      (None, 128, 32, 64)  0           ['block_9_conv_bn[0][0]']        \n",
      "                                                                                                  \n",
      " block_10_de-conv (Conv2DTransp  (None, 256, 64, 32)  18464      ['block_9_lrelu[0][0]']          \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " block_10_conv_bn (BatchNormali  (None, 256, 64, 32)  128        ['block_10_de-conv[0][0]']       \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " block_10_lrelu (LeakyReLU)     (None, 256, 64, 32)  0           ['block_10_conv_bn[0][0]']       \n",
      "                                                                                                  \n",
      " skip3 (Concatenate)            (None, 256, 64, 64)  0           ['block_10_lrelu[0][0]',         \n",
      "                                                                  'block_2_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_11_conv (Conv2D)         (None, 256, 64, 32)  18464       ['skip3[0][0]']                  \n",
      "                                                                                                  \n",
      " block_11_conv_bn (BatchNormali  (None, 256, 64, 32)  128        ['block_11_conv[0][0]']          \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " block_11_lrelu (LeakyReLU)     (None, 256, 64, 32)  0           ['block_11_conv_bn[0][0]']       \n",
      "                                                                                                  \n",
      " block_12_de-conv (Conv2DTransp  (None, 512, 128, 16  4624       ['block_11_lrelu[0][0]']         \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " block_12_conv_bn (BatchNormali  (None, 512, 128, 16  64         ['block_12_de-conv[0][0]']       \n",
      " zation)                        )                                                                 \n",
      "                                                                                                  \n",
      " block_12_lrelu (LeakyReLU)     (None, 512, 128, 16  0           ['block_12_conv_bn[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " skip4 (Concatenate)            (None, 512, 128, 32  0           ['block_12_lrelu[0][0]',         \n",
      "                                )                                 'block_1_lrelu[0][0]']          \n",
      "                                                                                                  \n",
      " block_13_conv (Conv2D)         (None, 512, 128, 1)  289         ['skip4[0][0]']                  \n",
      "                                                                                                  \n",
      " block_13_lrelu (LeakyReLU)     (None, 512, 128, 1)  0           ['block_13_conv[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,175,665\n",
      "Trainable params: 1,173,745\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "35/35 [==============================] - 15s 376ms/step - loss: 1382.1731 - accuracy: 0.0000e+00 - val_loss: 1465785088.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 115.0667 - accuracy: 0.0000e+00 - val_loss: 30157386.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 62.7584 - accuracy: 0.0000e+00 - val_loss: 943587.8750 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 37.0944 - accuracy: 0.0000e+00 - val_loss: 9298.5635 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 38.2590 - accuracy: 0.0000e+00 - val_loss: 1006.6312 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 33.0026 - accuracy: 0.0000e+00 - val_loss: 443.7823 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 32.3683 - accuracy: 0.0000e+00 - val_loss: 394.2176 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 28.2326 - accuracy: 0.0000e+00 - val_loss: 324.1184 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 21.9630 - accuracy: 0.0000e+00 - val_loss: 163.1689 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 23.4443 - accuracy: 0.0000e+00 - val_loss: 277.9532 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 31.8799 - accuracy: 0.0000e+00 - val_loss: 139.5363 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 18.2334 - accuracy: 0.0000e+00 - val_loss: 45.3552 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 23.6184 - accuracy: 0.0000e+00 - val_loss: 74.8096 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 32.7999 - accuracy: 0.0000e+00 - val_loss: 120.7991 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 24.5176 - accuracy: 0.0000e+00 - val_loss: 29.1877 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 19.3538 - accuracy: 0.0000e+00 - val_loss: 22.9185 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 20.1212 - accuracy: 0.0000e+00 - val_loss: 76.4303 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 24.3669 - accuracy: 0.0000e+00 - val_loss: 214.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 20.6843 - accuracy: 0.0000e+00 - val_loss: 24.8649 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 18.4470 - accuracy: 0.0000e+00 - val_loss: 24.8422 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 21.6436 - accuracy: 0.0000e+00 - val_loss: 13.3310 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 15.2181 - accuracy: 0.0000e+00 - val_loss: 21.2233 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 14.4372 - accuracy: 0.0000e+00 - val_loss: 32.6938 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 20.6291 - accuracy: 0.0000e+00 - val_loss: 26.1132 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 16.7646 - accuracy: 0.0000e+00 - val_loss: 17.3036 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 16.8932 - accuracy: 0.0000e+00 - val_loss: 212.5106 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 17.3007 - accuracy: 0.0000e+00 - val_loss: 81.2854 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 21.6848 - accuracy: 0.0000e+00 - val_loss: 18.8253 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 17.7329 - accuracy: 0.0000e+00 - val_loss: 18.8364 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.6648 - accuracy: 0.0000e+00 - val_loss: 32.1486 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 16.4708 - accuracy: 0.0000e+00 - val_loss: 13.9399 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.9003 - accuracy: 0.0000e+00 - val_loss: 33.2946 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 15.9379 - accuracy: 0.0000e+00 - val_loss: 56.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 20.2944 - accuracy: 0.0000e+00 - val_loss: 47.9401 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.1599 - accuracy: 0.0000e+00 - val_loss: 167.1338 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 15.1566 - accuracy: 0.0000e+00 - val_loss: 21.5020 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.2676 - accuracy: 0.0000e+00 - val_loss: 18.2419 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.8002 - accuracy: 0.0000e+00 - val_loss: 34.7760 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.9968 - accuracy: 0.0000e+00 - val_loss: 15.5356 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 12.9092 - accuracy: 0.0000e+00 - val_loss: 10.8206 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.4592 - accuracy: 0.0000e+00 - val_loss: 81.3821 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 23.6134 - accuracy: 0.0000e+00 - val_loss: 59.1076 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 13.8833 - accuracy: 0.0000e+00 - val_loss: 37.0747 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.2612 - accuracy: 0.0000e+00 - val_loss: 20.0707 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 17.9043 - accuracy: 0.0000e+00 - val_loss: 48.5055 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.3722 - accuracy: 0.0000e+00 - val_loss: 180.6908 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 13.5391 - accuracy: 0.0000e+00 - val_loss: 19.3953 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.8252 - accuracy: 0.0000e+00 - val_loss: 11.9869 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 18.0230 - accuracy: 0.0000e+00 - val_loss: 21.8547 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.5335 - accuracy: 0.0000e+00 - val_loss: 12.3882 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.4007 - accuracy: 0.0000e+00 - val_loss: 14.5106 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 21.8470 - accuracy: 0.0000e+00 - val_loss: 22.0681 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 29.9323 - accuracy: 0.0000e+00 - val_loss: 56.4661 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 17.2740 - accuracy: 0.0000e+00 - val_loss: 63.8308 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 16.8934 - accuracy: 0.0000e+00 - val_loss: 31.1459 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 22.2073 - accuracy: 0.0000e+00 - val_loss: 30.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 15.6120 - accuracy: 0.0000e+00 - val_loss: 19.4482 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 22.4185 - accuracy: 0.0000e+00 - val_loss: 30.9538 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 29.5254 - accuracy: 0.0000e+00 - val_loss: 1648.4460 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 16.0366 - accuracy: 0.0000e+00 - val_loss: 2209.6423 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 21.4513 - accuracy: 0.0000e+00 - val_loss: 34.7947 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 16.1019 - accuracy: 0.0000e+00 - val_loss: 16.8118 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 20.5506 - accuracy: 0.0000e+00 - val_loss: 116.3169 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.1829 - accuracy: 0.0000e+00 - val_loss: 35.0965 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.4766 - accuracy: 0.0000e+00 - val_loss: 12.3962 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 9.8046 - accuracy: 0.0000e+00 - val_loss: 33.2318 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 18.2096 - accuracy: 0.0000e+00 - val_loss: 17.3369 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 15.0725 - accuracy: 0.0000e+00 - val_loss: 60.8304 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 15.6910 - accuracy: 0.0000e+00 - val_loss: 69.2576 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 15.2733 - accuracy: 0.0000e+00 - val_loss: 15.7339 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 18.4693 - accuracy: 0.0000e+00 - val_loss: 88.8623 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 24.7680 - accuracy: 0.0000e+00 - val_loss: 181.1221 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 17.7029 - accuracy: 0.0000e+00 - val_loss: 36.6652 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.4787 - accuracy: 0.0000e+00 - val_loss: 10.9783 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 10.8923 - accuracy: 0.0000e+00 - val_loss: 11.3110 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 12.6039 - accuracy: 0.0000e+00 - val_loss: 20.4568 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 13.5976 - accuracy: 0.0000e+00 - val_loss: 29.0765 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.8049 - accuracy: 0.0000e+00 - val_loss: 12.9948 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 16.9039 - accuracy: 0.0000e+00 - val_loss: 32.6781 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 31.6849 - accuracy: 0.0000e+00 - val_loss: 352.4606 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 48.4973 - accuracy: 0.0000e+00 - val_loss: 14628.6689 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "35/35 [==============================] - 13s 362ms/step - loss: 33.0948 - accuracy: 0.0000e+00 - val_loss: 1522.5358 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 21.9402 - accuracy: 0.0000e+00 - val_loss: 341.0897 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.9318 - accuracy: 0.0000e+00 - val_loss: 64.7724 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.3857 - accuracy: 0.0000e+00 - val_loss: 28.4011 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "35/35 [==============================] - 13s 362ms/step - loss: 17.8361 - accuracy: 0.0000e+00 - val_loss: 29.1796 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 19.4227 - accuracy: 0.0000e+00 - val_loss: 45.3507 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 19.6542 - accuracy: 0.0000e+00 - val_loss: 21.1618 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 13.3417 - accuracy: 0.0000e+00 - val_loss: 48.6332 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.5736 - accuracy: 0.0000e+00 - val_loss: 36.1676 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 14.4022 - accuracy: 0.0000e+00 - val_loss: 22.1677 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 15.0045 - accuracy: 0.0000e+00 - val_loss: 22.3203 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "35/35 [==============================] - 13s 362ms/step - loss: 14.0213 - accuracy: 0.0000e+00 - val_loss: 19.4516 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 12.9113 - accuracy: 0.0000e+00 - val_loss: 11.9583 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.0630 - accuracy: 0.0000e+00 - val_loss: 10.2680 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 17.0131 - accuracy: 0.0000e+00 - val_loss: 95.4235 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 17.3042 - accuracy: 0.0000e+00 - val_loss: 116.3355 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 15.4171 - accuracy: 0.0000e+00 - val_loss: 139.3749 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.8063 - accuracy: 0.0000e+00 - val_loss: 38.0717 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.6746 - accuracy: 0.0000e+00 - val_loss: 39.8325 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 21.2785 - accuracy: 0.0000e+00 - val_loss: 25.3002 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 17.5521 - accuracy: 0.0000e+00 - val_loss: 22.5189 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 14.6225 - accuracy: 0.0000e+00 - val_loss: 25.4241 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 18.4014 - accuracy: 0.0000e+00 - val_loss: 12.6757 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 12.8808 - accuracy: 0.0000e+00 - val_loss: 69.5171 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.6225 - accuracy: 0.0000e+00 - val_loss: 15.4097 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 13.3488 - accuracy: 0.0000e+00 - val_loss: 58.5955 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 30.4877 - accuracy: 0.0000e+00 - val_loss: 188.5990 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 34.0444 - accuracy: 0.0000e+00 - val_loss: 183.6617 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 23.5896 - accuracy: 0.0000e+00 - val_loss: 180.8657 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 19.0944 - accuracy: 0.0000e+00 - val_loss: 93.9692 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 23.8413 - accuracy: 0.0000e+00 - val_loss: 39.9250 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 20.3563 - accuracy: 0.0000e+00 - val_loss: 22.8144 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 17.6716 - accuracy: 0.0000e+00 - val_loss: 55.3760 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 15.2403 - accuracy: 0.0000e+00 - val_loss: 94.2687 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 20.2575 - accuracy: 0.0000e+00 - val_loss: 49.5641 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 21.0355 - accuracy: 0.0000e+00 - val_loss: 58.8096 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 228066.8281 - accuracy: 0.0000e+00 - val_loss: 1396144340992.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "35/35 [==============================] - 13s 361ms/step - loss: 420204.2188 - accuracy: 0.0000e+00 - val_loss: 872087040.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 1967.6456 - accuracy: 0.0000e+00 - val_loss: 15944493.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 232.7882 - accuracy: 0.0000e+00 - val_loss: 529703.3750 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 183.5836 - accuracy: 0.0000e+00 - val_loss: 24358.8906 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 170.4688 - accuracy: 0.0000e+00 - val_loss: 1347.4816 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 164.4063 - accuracy: 0.0000e+00 - val_loss: 336.1745 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 160.5186 - accuracy: 0.0000e+00 - val_loss: 249.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 158.6162 - accuracy: 0.0000e+00 - val_loss: 170.9165 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 150.6702 - accuracy: 0.0000e+00 - val_loss: 200.7573 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 147.7769 - accuracy: 0.0000e+00 - val_loss: 166.7624 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 145.3622 - accuracy: 0.0000e+00 - val_loss: 186.7408 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 141.4817 - accuracy: 0.0000e+00 - val_loss: 143.1501 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 140.1020 - accuracy: 0.0000e+00 - val_loss: 150.2159 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "35/35 [==============================] - 13s 360ms/step - loss: 135.0514 - accuracy: 0.0000e+00 - val_loss: 130.5333 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 134.0273 - accuracy: 0.0000e+00 - val_loss: 142.8226 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 129.4227 - accuracy: 0.0000e+00 - val_loss: 125.0508 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 130.1608 - accuracy: 0.0000e+00 - val_loss: 120.4306 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 127.1428 - accuracy: 0.0000e+00 - val_loss: 126.2572 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 120.5247 - accuracy: 0.0000e+00 - val_loss: 134.8129 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 114.8468 - accuracy: 0.0000e+00 - val_loss: 141.0752 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 111.2451 - accuracy: 0.0000e+00 - val_loss: 115.4924 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 105.1164 - accuracy: 0.0000e+00 - val_loss: 125.6208 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 101.5516 - accuracy: 0.0000e+00 - val_loss: 108.5613 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 97.3738 - accuracy: 0.0000e+00 - val_loss: 155.3946 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 90.2575 - accuracy: 0.0000e+00 - val_loss: 150.1742 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 80.8846 - accuracy: 0.0000e+00 - val_loss: 118.8973 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 76.3751 - accuracy: 0.0000e+00 - val_loss: 129.1595 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 66.8032 - accuracy: 0.0000e+00 - val_loss: 141.7414 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 58.4288 - accuracy: 0.0000e+00 - val_loss: 140.8365 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "35/35 [==============================] - 13s 359ms/step - loss: 52.0782 - accuracy: 0.0000e+00 - val_loss: 92.9570 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 46.4105 - accuracy: 0.0000e+00 - val_loss: 53.8546 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "35/35 [==============================] - 13s 358ms/step - loss: 44.8873 - accuracy: 0.0000e+00 - val_loss: 55.3929 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 00:15:03,529 - INFO - save_model -> /notebooks/model/model_car4.hdf5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loss_plot() missing 1 required positional argument: 'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_file_path)\n\u001b[1;32m     37\u001b[0m com\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_model -> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_file_path))\n\u001b[0;32m---> 38\u001b[0m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m visualizer\u001b[38;5;241m.\u001b[39msave_figure(history_img)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============== END TRAINING ==============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_plot() missing 1 required positional argument: 'val_loss'"
     ]
    }
   ],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car5\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4.5)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car6\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1224, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1224, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4.5)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car7\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4.5)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car8\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                   \n",
    "                         n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1216, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1216, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4.5)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "# visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "# visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car1\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1210, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1210, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4.5)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "# visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "# visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car2\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "data_masked, data = masking(data)\n",
    "data = data.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "data_masked =  data_masked.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=data_masked,\n",
    "                    y=data,\n",
    "                    epochs=150,\n",
    "                    batch_size=32,\n",
    "                    shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                    validation_split=0.1,\n",
    "                    verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "# visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "# visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7d30985f23d0c7700ff9117488976e591efd57aed74d4a8ca9ee5ad3f3e7e0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

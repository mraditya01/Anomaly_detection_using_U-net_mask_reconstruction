{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.23.1)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.56.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.8.1)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soundfile>=0.10.2\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.1.1)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setuptools<60\n",
      "  Downloading setuptools-59.8.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.8/952.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23702 sha256=bf49b2d0d09e5e1fa1bf739ca18bbd4cc42e0f1910994fdffa38f7164663f90e\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/76/a4/cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "Successfully built audioread\n",
      "Installing collected packages: appdirs, setuptools, llvmlite, audioread, soundfile, pooch, numba, resampy, librosa\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.1.0\n",
      "    Uninstalling setuptools-63.1.0:\n",
      "      Successfully uninstalled setuptools-63.1.0\n",
      "Successfully installed appdirs-1.4.4 audioread-3.0.0 librosa-0.9.2 llvmlite-0.39.1 numba-0.56.2 pooch-1.6.0 resampy-0.4.2 setuptools-59.8.0 soundfile-0.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting kapre\n",
      "  Downloading kapre-0.3.7.tar.gz (26 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from kapre) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from kapre) (1.23.1)\n",
      "Requirement already satisfied: librosa>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from kapre) (0.9.2)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (0.11.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (0.56.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (1.8.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (3.0.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (1.6.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (0.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.7.2->kapre) (5.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (3.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (59.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.47.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (2.9.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.1.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (3.19.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.0.0->kapre) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (4.3.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (14.0.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (2.9.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (0.26.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0.0->kapre) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->kapre) (0.35.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.45.1->librosa>=0.7.2->kapre) (0.39.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->librosa>=0.7.2->kapre) (3.0.9)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa>=0.7.2->kapre) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa>=0.7.2->kapre) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->librosa>=0.7.2->kapre) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa>=0.7.2->kapre) (1.15.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (0.6.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.7.2->kapre) (2.21)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (4.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0->kapre) (3.2.0)\n",
      "Building wheels for collected packages: kapre\n",
      "  Building wheel for kapre (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kapre: filename=kapre-0.3.7-py3-none-any.whl size=29625 sha256=ec1b999f19efa1650e2b7e5c862db34bd59a9050b61aa728dd2c88ce28264577\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/04/6b/a7c14b363e7942dad0706127cbf5b5817e51010175fda9026a\n",
      "Successfully built kapre\n",
      "Installing collected packages: kapre\n",
      "Successfully installed kapre-0.3.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# import default libraries\n",
    "########################################################################\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional libraries\n",
    "########################################################################\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "# from import\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "except:\n",
    "    import joblib\n",
    "# original lib\n",
    "import common as com\n",
    "import keras_model\n",
    "import pandas as pd\n",
    "import keras\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "from generator import SpecAugment\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# load parameter.yaml\n",
    "########################################################################\n",
    "param = com.yaml_load()\n",
    "########################################################################\n",
    "saved_weight = os.path.join(param[\"P_MODELSAVE\"], 'dataweights.{epoch:02d}-{val_acc:.2f}.hdf5')\n",
    "\n",
    "modelchk = keras.callbacks.ModelCheckpoint(saved_weight, \n",
    "                                      monitor='val_acc', \n",
    "                                      verbose=1,\n",
    "                                      save_best_only=True, \n",
    "                                      save_weights_only=False,\n",
    "                                      mode='auto',\n",
    "                                      period=2)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=param[\"P_LOGS\"],\n",
    "                                          histogram_freq=0,\n",
    "                                          write_graph=True,\n",
    "                                          write_images=True)\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(f'{param[\"P_LOGS\"]}/keras_log.csv',\n",
    "                                       append=True)\n",
    "\n",
    "# model_unet = keras_model.get_model(input_shape=(1,param[\"feature\"][\"n_mels\"],param[\"feature\"][\"n_frames\"]), lr = param[\"fit\"][\"lr\"])\n",
    "\n",
    "########################################################################\n",
    "# visualizer\n",
    "########################################################################\n",
    "class visualizer(object):\n",
    "    def __init__(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.plt = plt\n",
    "        self.fig = self.plt.figure(figsize=(7, 5))\n",
    "        self.plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    def loss_plot(self, loss, val_loss):\n",
    "        \"\"\"\n",
    "        Plot loss curve.\n",
    "\n",
    "        loss : list [ float ]\n",
    "            training loss time series.\n",
    "        val_loss : list [ float ]\n",
    "            validation loss time series.\n",
    "\n",
    "        return   : None\n",
    "        \"\"\"\n",
    "        ax = self.fig.add_subplot(1, 1, 1)\n",
    "        ax.cla()\n",
    "        ax.plot(loss)\n",
    "        ax.plot(val_loss)\n",
    "        ax.set_title(\"Model loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "    def save_figure(self, name):\n",
    "        \"\"\"\n",
    "        Save figure.\n",
    "\n",
    "        name : str\n",
    "            save png file path.\n",
    "\n",
    "        return : None\n",
    "        \"\"\"\n",
    "        self.plt.savefig(name)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# get data from the list for file paths\n",
    "########################################################################\n",
    "def file_list_to_data(file_list,\n",
    "                      msg=\"calc...\",\n",
    "                      n_mels=64,\n",
    "                      n_frames=5,\n",
    "                      n_hop_frames=1,\n",
    "                      n_fft=1024,\n",
    "                      hop_length=512,\n",
    "                      power=2.0):\n",
    "    \"\"\"\n",
    "    convert the file_list to a vector array.\n",
    "    file_to_vector_array() is iterated, and the output vector array is concatenated.\n",
    "\n",
    "    file_list : list [ str ]\n",
    "        .wav filename list of dataset\n",
    "    msg : str ( default = \"calc...\" )\n",
    "        description for tqdm.\n",
    "        this parameter will be input into \"desc\" param at tqdm.\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        data for training (this function is not used for test.)\n",
    "        * dataset.shape = (number of feature vectors, dimensions of feature vectors)\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate file_to_vector_array()\n",
    "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
    "        vectors = com.file_to_vectors(file_list[idx],\n",
    "                                                n_mels=n_mels,\n",
    "                                                n_frames=n_frames,\n",
    "                                                n_fft=n_fft,\n",
    "                                                hop_length=hop_length,\n",
    "                                                power=power)\n",
    "        # vectors_masked = com.spec_augment(vectors)\n",
    "        if idx == 0:\n",
    "            data = np.zeros((len(file_list), vectors.shape[0], vectors.shape[1]), float)\n",
    "            # data_masked = np.zeros((len(file_list), vectors_masked.shape[1], vectors_masked.shape[0]), float)\n",
    "        data[idx, :, :] = vectors\n",
    "        data[idx,:,:] = librosa.power_to_db(data[idx,:,:])\n",
    "        # data_masked[idx, :, :] = vectors_masked.T\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 10:09:54,775 - INFO - load_directory <- development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "[1/8] dev_data/car3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mode = True\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(param[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "# initialize the visualizer\n",
    "visualizer = visualizer()\n",
    "\n",
    "# load base_directory list\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "idx = 0\n",
    "# target_dir = \"/content/drive/MyDrive/dev_data/car8\"\n",
    "target_dir = \"dev_data/car3\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "print(\"[{idx}/{total}] {target_dir}\".format(target_dir=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "# set path\n",
    "machine_type = os.path.split(target_dir)[1]\n",
    "model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=param[\"model_directory\"],\n",
    "                                                                machine_type=machine_type)\n",
    "\n",
    "history_img = \"{model}/history_{machine_type}.png\".format(model=param[\"model_directory\"],\n",
    "                                                            machine_type=machine_type)\n",
    "# pickle file for storing anomaly score distribution\n",
    "score_distr_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(model=param[\"model_directory\"],\n",
    "                                                                        machine_type=machine_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 10:09:54,789 - INFO - target_dir : dev_data/car3_*\n",
      "2022-10-11 10:09:54,797 - INFO - #files : 611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DATASET_GENERATOR ==============\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████| 611/611 [00:12<00:00, 47.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "files, y_true = com.file_list_generator(target_dir=target_dir,\n",
    "                                        section_name=\"*\",\n",
    "                                        dir_name=\"train\",\n",
    "                                        mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "data = file_list_to_data(files,\n",
    "                            msg=\"generate train_dataset\",\n",
    "                            n_mels=param[\"feature\"][\"n_mels\"],\n",
    "                            n_frames=1,\n",
    "                            n_hop_frames=param[\"feature\"][\"n_mels\"],\n",
    "                            n_fft=param[\"feature\"][\"n_fft\"],\n",
    "                            hop_length=param[\"feature\"][\"hop_length\"],\n",
    "                            power=param[\"feature\"][\"power\"])\n",
    "data = np.concatenate((data, data), axis=0)\n",
    "j = param[\"feature\"][\"n_mels\"]\n",
    "y = param[\"feature\"][\"n_frames\"]\n",
    "n = machine_type\n",
    "print(data.shape)\n",
    "# data_masked, data = masking(data)\n",
    "# data = data.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "# data_masked =  data_masked.reshape(1222, data.shape[1], data.shape[2], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# j = param[\"feature\"][\"n_mels\"]\n",
    "# y = param[\"feature\"][\"n_frames\"]\n",
    "# data = np.load(f\"data__{j}_{y}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 32, 8, 128), (None, 64, 16, 128)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# number of vectors for each wave file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============== MODEL TRAINING ==============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_frames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_mels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model_opt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mmodel_opt, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/notebooks/keras_model.py:71\u001b[0m, in \u001b[0;36mget_model_3\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     66\u001b[0m conv5 \u001b[38;5;241m=\u001b[39m Conv2DLayer(maxpool4, \u001b[38;5;241m256\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, block_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     69\u001b[0m deconv1 \u001b[38;5;241m=\u001b[39m Transpose_Conv2D(conv5, \u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, block_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m skip1 \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdeconv1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m conv7 \u001b[38;5;241m=\u001b[39m Conv2DLayer(skip1, \u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, block_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     73\u001b[0m deconv2 \u001b[38;5;241m=\u001b[39m Transpose_Conv2D(conv7, \u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, block_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/layers/merging/concatenate.py:217\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.layers.concatenate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;124;03m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m  >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m      A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/layers/merging/concatenate.py:123\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    120\u001b[0m unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    121\u001b[0m     shape[axis] \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 32, 8, 128), (None, 64, 16, 128)]"
     ]
    }
   ],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model.get_model_3((param[\"feature\"][\"n_frames\"],param[\"feature\"][\"n_mels\"], 1))\n",
    "model_opt = tf.keras.optimizers.Adam(learning_rate=4)\n",
    "\n",
    "model.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# history = model.fit(x=data_masked,\n",
    "#                     y=data,\n",
    "#                     epochs=200,\n",
    "#                     batch_size=32,\n",
    "#                     shuffle=param[\"fit\"][\"shuffle\"],\n",
    "#                     validation_split=0.1,\n",
    "#                     verbose=param[\"fit\"][\"verbose\"])\n",
    "\n",
    "history = model.fit(SpecAugment(data, 32),\n",
    "                 epochs=200,\n",
    "                 shuffle=param[\"fit\"][\"shuffle\"],\n",
    "                 verbose=param[\"fit\"][\"verbose\"],\n",
    "                 workers = 6)\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}.csv',index=False)\n",
    "\n",
    "\n",
    "# # calculate y_pred for fitting anomaly score distribution\n",
    "# y_pred = []\n",
    "# start_idx = 0\n",
    "# for file_idx in range(len(files)):\n",
    "#         y_pred.append(np.mean(np.square(data[file_idx,: ,  :] \n",
    "#                                 - model.predict(data[file_idx,: , :]))))\n",
    "#         start_idx += n_vectors_ea_file\n",
    "\n",
    "# # fit anomaly score distribution\n",
    "# shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(y_pred)\n",
    "# gamma_params = [shape_hat, loc_hat, scale_hat]\n",
    "# joblib.dump(gamma_params, score_distr_file_path)\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "visualizer.loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "visualizer.save_figure(history_img)\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "del data\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7d30985f23d0c7700ff9117488976e591efd57aed74d4a8ca9ee5ad3f3e7e0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
